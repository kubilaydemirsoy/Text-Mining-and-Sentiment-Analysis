{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cZhLkmUkO59g"
   },
   "outputs": [],
   "source": [
    "# Import PyDrive and associated libraries\n",
    "# This only needs to be done once per notebook\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Authenticate and create the PyDrive client\n",
    "# This only needs to be done once per notebook\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SESTuTU_T8WO"
   },
   "source": [
    "* Specify the Google Drive file ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rxz1za_qBYRt"
   },
   "outputs": [],
   "source": [
    "# Download a file based on its file ID.\n",
    "\n",
    "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
    "file_id = 'YOUR_FILE_ID' # Check your own ID in GDrive\n",
    "downloaded = drive.CreateFile({'id': file_id})\n",
    "\n",
    "# Save file in Colab memory\n",
    "downloaded.GetContentFile('tweet_data.csv')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8IZvvYyA9sr"
   },
   "source": [
    "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=250px>\n",
    "\n",
    "## **1.2** Load and analyze Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1SvV_1MAXMq"
   },
   "source": [
    "Very useful packages for Data Analysis. Don't hesitate to visit any of those sites in case you don't get what we are doing.\n",
    "*   `Pandas`: https://pandas.pydata.org/docs/\n",
    "*   `Numpy`: https://numpy.org/doc/\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-K4rZaHvkJhd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmpTLrUWAqFI"
   },
   "source": [
    "* Read Dataframe stored in Google Drive under `.csv` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BMVYdAbhkSEv"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"tweet_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxeev34FBIJH"
   },
   "source": [
    "* Use the `sample` method to look at some random tweets present in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X_ePfKnQkiHh"
   },
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKPIS_XzCCkO"
   },
   "source": [
    "* Check how many tweets there are in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-d8KD2RCzBMv"
   },
   "outputs": [],
   "source": [
    "print(\"Number of tweets: {}\".format(len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "US5Uqnn4V1RT"
   },
   "source": [
    "* Print a tweet and its sentiment based on a tweet ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UeDUPoASy8Bh"
   },
   "outputs": [],
   "source": [
    "tweet_id = 4879\n",
    "tweet = df.iloc[tweet_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nKuB8wZ4ftXE"
   },
   "outputs": [],
   "source": [
    "print(\"Tweet: {}\".format(tweet[\"tweet_text\"]))\n",
    "print(\"Tweet sentiment: {}\".format(tweet[\"sentiment\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2QDfi2dWWFK"
   },
   "source": [
    "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=250px>\n",
    "\n",
    "## **1.3** Dataset Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDpxKjeZQcKE"
   },
   "source": [
    "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **1.3.1** Matplotlib</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DPCRFyGCuMc"
   },
   "source": [
    "* Import the `pyplot` module from the matplotlib package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fb79SIovB06O"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-sMh0LQRV_Da"
   },
   "source": [
    "* `pyplot` helps understanding and representing how tweets are distributed over the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ya7k2TcylAbp"
   },
   "outputs": [],
   "source": [
    "sentiment_count = df[\"sentiment\"].value_counts()\n",
    "plt.pie(sentiment_count, labels=sentiment_count.index,\n",
    "        autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFvGFDbFRZGK"
   },
   "source": [
    "* Print the count of positive and negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MlwWqCohC87z"
   },
   "outputs": [],
   "source": [
    "print(\"Number of + tweets: {}\".format(df[df[\"sentiment\"]==\"positive\"].count()[0]))\n",
    "print(\"Number of - tweets: {}\".format(df[df[\"sentiment\"]==\"negative\"].count()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcXAMS4bQjGM"
   },
   "source": [
    "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **1.3.2** Wordclouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTIV2ElsBhZk"
   },
   "source": [
    "* The `Wordclouds` package is very useful to get a quick overview of most recurrent words in the text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LiME9k4uBxix"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Stf9-UtnNloU"
   },
   "source": [
    "* What are the words most often present in positive tweets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fW3YNWHwwYdP"
   },
   "outputs": [],
   "source": [
    "pos_tweets = df[df[\"sentiment\"]==\"positive\"]\n",
    "txt = \" \".join(tweet.lower() for tweet in pos_tweets[\"tweet_text\"])\n",
    "wordcloud = WordCloud().generate(txt)\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7FsFmH6NqWy"
   },
   "source": [
    "* ... and in negative tweets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MbHSPX8jE2Jk"
   },
   "outputs": [],
   "source": [
    "neg_tweets = df[df[\"sentiment\"]==\"negative\"]\n",
    "txt = \" \".join(tweet.lower() for tweet in neg_tweets[\"tweet_text\"])\n",
    "wordcloud = WordCloud().generate(txt)\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIFAbVfhMvKS"
   },
   "source": [
    "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=650px>\n",
    "\n",
    "# `SECTION 2` Text Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56HdmPCjMjls"
   },
   "source": [
    "As this section relies quite a lot on `REGEX`, you might want to have a look at the following website: https://regexr.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8D9TW-jHaDMY"
   },
   "source": [
    "* Import `regex` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qefx0Ad1RFHy"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKiiuRQpdxjm"
   },
   "source": [
    "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=250px>\n",
    "\n",
    "## **2.1** Twitter features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1N3d1u49GUt"
   },
   "source": [
    "* Example of a *random* tweet that can be found on Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8qaWEnedWB1"
   },
   "outputs": [],
   "source": [
    "tweet = \"RT @AIOutsider I love this! üëç https://AIOutsider.com #NLP #Fun\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NaKqk-_gFQQN"
   },
   "source": [
    "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **2.1.1** RT Tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHH-1YKcIuRW"
   },
   "source": [
    "#### Need a hint?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZVeoDQK6kvr"
   },
   "source": [
    "* `R` : match \"R\" character\n",
    "* `T` : match \"T\" character\n",
    "* `\\s` : match any whitespace character\n",
    "* `+` : match one or more of the preceding tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMrR3STEfdrA"
   },
   "source": [
    "#### Handle the RT Tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5R1W_qw3AZs"
   },
   "source": [
    "* Replace occurences of `RT` with a default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RMe1sJQZWBon"
   },
   "outputs": [],
   "source": [
    "def replace_retweet(tweet, default_replace=\"\"):\n",
    "  tweet = re.sub('RT\\s+', default_replace, tweet)\n",
    "  return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AIFwuvPQI5L4"
   },
   "outputs": [],
   "source": [
    "print(\"Processed tweet: {}\".format(replace_retweet(tweet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uY48gGm2fgtV"
   },
   "source": [
    "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **2.1.2** @User Tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LKMid597qjL"
   },
   "source": [
    "#### Need a hint?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uc0j6Mfw8Tcw"
   },
   "source": [
    "* `\\B` : match any position that is not a word boundary\n",
    "* `@` : match \"@\" character\n",
    "* `\\w` : match any word character \n",
    "* `+` : match one or more of the preceding tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yjn3WP56Dc-O"
   },
   "source": [
    "#### Handle the User Tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DMdteLL3Sq-"
   },
   "source": [
    "* Replace `@_Someone_` with a default user tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ODnCCX1ueIwO"
   },
   "outputs": [],
   "source": [
    "def replace_user(tweet, default_replace=\"twitteruser\"):\n",
    "  tweet = re.sub('\\B@\\w+', default_replace, tweet)\n",
    "  return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wgSircywI8fG"
   },
   "outputs": [],
   "source": [
    "print(\"Processed tweet: {}\".format(replace_user(tweet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QsbZqz2Cn1dB"
   },
   "source": [
    "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **2.1.3** Emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xE5IZIx5Gkx4"
   },
   "source": [
    "* Install the `emoji` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PDtbHjDiiaw7"
   },
   "outputs": [],
   "source": [
    "pip install emoji --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTSClFfP3iXm"
   },
   "source": [
    "* Import the installed package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T8Ctx6FUiVyc"
   },
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pZp0rcEGtH8"
   },
   "source": [
    "* Replace emojis with a meaningful text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hqHXnwRZn2Ey"
   },
   "outputs": [],
   "source": [
    "def demojize(tweet):\n",
    "  tweet = emoji.demojize(tweet)\n",
    "  return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NeuCWELwx2bb"
   },
   "outputs": [],
   "source": [
    "print(\"Processed tweet: {}\".format(demojize(tweet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSYc3ax5fk5m"
   },
   "source": [
    "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **2.1.4** URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7URUz-xz9otD"
   },
   "source": [
    "#### Need a hint?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKaWmYHL4bmw"
   },
   "source": [
    "* `(http|https)` : capturing group matching either http or https\n",
    "* `:` : match the \":\" character\n",
    "* `\\/` : match the \"/\" charachter\n",
    "* `\\S` : match any character that is not whitespace\n",
    "* `+` : match one or more of the preceding tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Y1_YIxt5z3d"
   },
   "source": [
    "#### Handle the URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gp8E1W-e4QDT"
   },
   "source": [
    "* Replace occurences of `http://` or `https://` with a default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Vq3d5VcfdCc"
   },
   "outputs": [],
   "source": [
    "def replace_url(tweet, default_replace=\"\"):\n",
    "  tweet = re.sub('(http|https):\\/\\/\\S+', default_replace, tweet)\n",
    "  return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YN2c4pJIy8pX"
   },
   "outputs": [],
   "source": [
    "print(\"Processed tweet: {}\".format(replace_url(tweet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piKIdmGCimZ1"
   },
   "source": [
    "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **2.1.5** Hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yePSPCRy4WWk"
   },
   "source": [
    "* Replace occurences of `#_something_` with a default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yI76Ve3lg3WZ"
   },
   "outputs": [],
   "source": [
    "def replace_hashtag(tweet, default_replace=\"\"):\n",
    "  tweet = re.sub('#+', default_replace, tweet)\n",
    "  return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1xlMKyD4y-Xg"
   },
   "outputs": [],
   "source": [
    "print(\"Processed tweet: {}\".format(replace_hashtag(tweet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77D-JqISVFvi"
   },
   "source": [
    "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=250px>\n",
    "\n",
    "## **2.2** Word Features\n",
    "\n",
    "Let's now have a look at some other features that are not really Twitter-dependant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xw18TAaFhOMH"
   },
   "outputs": [],
   "source": [
    "tweet = \"LOOOOOOOOK at this ... I'd like it so much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmN1OTYBi8r3"
   },
   "source": [
    "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **2.2.1** Remove upper capitalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noY_ATKH4hcz"
   },
   "source": [
    "* Lower case each letter in a specific tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXCITDNeQtnE"
   },
   "outputs": [],
   "source": [
    "def to_lowercase(tweet):\n",
    "  tweet = tweet.lower()\n",
    "  return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Roo8SHo23vw"
   },
   "outputs": [],
   "source": [
    "print(\"Processed tweet: {}\".format(to_lowercase(tweet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNlZxI4ZS-dX"
   },
   "source": [
    "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **2.2.2** Word repetition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKkeEGaX4m40"
   },
   "source": [
    "* Replace word repetition with a single occurence (\"oooooo\" becomes \"oo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "USLuHaaiS9xm"
   },
   "outputs": [],
   "source": [
    "def word_repetition(tweet):\n",
    "  tweet = re.sub(r'(.)\\1+', r'\\1\\1', tweet)\n",
    "  return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CkWh38hH5qyY"
   },
   "outputs": [],
   "source": [
    "print(\"Processed tweet: {}\".format(word_repetition(tweet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qli_4tnBpcr4"
   },
   "source": [
    "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **2.2.3** Punctuation repetition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQN36n3w4wNA"
   },
   "source": [
    "* Replace punctuation repetition with a single occurence (\"!!!!!\" becomes \"!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_awIiJ_ngo7"
   },
   "outputs": [],
   "source": [
    "def punct_repetition(tweet, default_replace=\"\"):\n",
    "  tweet = re.sub(r'[\\?\\.\\!]+(?=[\\?\\.\\!])', default_replace, tweet)\n",
    "  return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jfRfogEe5str"
   },
   "outputs": [],
   "source": [
    "print(\"Processed tweet: {}\".format(punct_repetition(tweet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpwgMqIxi6G9"
   },
   "source": [
    "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **2.2.4** Word contraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Adsu8A-TGFT5"
   },
   "source": [
    "* Install the `contractions` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wO2Gle7Ey360"
   },
   "outputs": [],
   "source": [
    "pip install contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOPWYcd146Rl"
   },
   "source": [
    "* Import the installed package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UdkPTEZryzJC"
   },
   "outputs": [],
   "source": [
    "import contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDGBT323GE1v"
   },
   "source": [
    "* Use `contractions_dict` to list most common contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4DwhVxKs1QL_"
   },
   "outputs": [],
   "source": [
    "print(contractions.contractions_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lho1c8I5D2B"
   },
   "source": [
    "* Create a `_fix_contractions` function used to replace contractions with their extended forms by using the contractions dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1xj5UXbVspq"
   },
   "outputs": [],
   "source": [
    "def _fix_contractions(tweet):\n",
    "  for k, v in contractions.contractions_dict.items():\n",
    "    tweet = tweet.replace(k, v)\n",
    "  return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2MDFOXKTccRV"
   },
   "outputs": [],
   "source": [
    "print(\"Processed tweet: {}\".format(_fix_contractions(tweet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQijW6z65IzJ"
   },
   "source": [
    "* Create a `_fix_contractions` function used to replace contractions with their extended forms by using the contractions package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MP6atIK-VsNw"
   },
   "outputs": [],
   "source": [
    "def fix_contractions(tweet):\n",
    "  tweet = contractions.fix(tweet)\n",
    "  return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MqQ60KaMc0CS"
   },
   "outputs": [],
   "source": [
    "print(\"Processed tweet: {}\".format(fix_contractions(tweet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7Qnxa-_2q6x"
   },
   "source": [
    "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=250px>\n",
    "\n",
    "## **2.3** Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eECnMHD_89wz"
   },
   "source": [
    "More information about NLTK? Head over to: https://www.nltk.org\n",
    "\n",
    "* Install the `NLTK` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SnKD-MkZ2kNd"
   },
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8KTZgynDSTg"
   },
   "source": [
    "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **2.3.1** Easy Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mO_sTCV19KA5"
   },
   "source": [
    "* Import `NLTK`\n",
    "* Import the `word_tokenize` module from NLTK \n",
    "* Download the `Punkt` tokenizer model from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-6SVKvVP4NDL"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCtkPkWIEXdl"
   },
   "source": [
    "* Simple tweet to be tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hJLYCxJztrXo"
   },
   "outputs": [],
   "source": [
    "tweet = \"These are 5 different words!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zo0nDxqUDy6s"
   },
   "source": [
    "* Create a `tokenize()` function that takes a tweet as input and returns a list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MthNx7ou4OnW"
   },
   "outputs": [],
   "source": [
    "def tokenize(tweet):\n",
    "  tokens = word_tokenize(tweet)\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DoiTPGFNDqAf"
   },
   "source": [
    "* Use the `tokenize()` function to print the tokenized version of a tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oPdJeqcRCYT2"
   },
   "outputs": [],
   "source": [
    "print(type(tokenize(tweet)))\n",
    "print(\"Tweet tokens: {}\".format(tokenize(tweet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3Ha_2Dtjl6X"
   },
   "source": [
    "That's it for the easy method! You will now see there is more you can do about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8KrFT2jDaVj"
   },
   "source": [
    "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **2.3.2** Custom Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLPhIt14FfVc"
   },
   "source": [
    "* Import the `string` package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "baAGSohY5PcV"
   },
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTwnZoFYTrpN"
   },
   "source": [
    "* Retrieve english punctuation signs by using the `string` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yc6naeSUkNs8"
   },
   "outputs": [],
   "source": [
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07KM4_kdUFsP"
   },
   "source": [
    "* Import the `stopwords` module from NLTK\n",
    "* Download `stopwords` data from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2MBBQJDeQSi4"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXQ8YUNj5-B1"
   },
   "source": [
    "* Create a set of english stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZXqXrG42qMk"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9G7T6Wg6A5F"
   },
   "source": [
    "* Remove some stopwords from the set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kcgpw29wmec4"
   },
   "outputs": [],
   "source": [
    "stop_words.discard('not')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4LLuBGH6Dy5"
   },
   "source": [
    "* Create a `custom_tokenize` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b6cZbaTB5Yr7"
   },
   "outputs": [],
   "source": [
    "def custom_tokenize(tweet,\n",
    "                    keep_punct = False,\n",
    "                    keep_alnum = False,\n",
    "                    keep_stop = False):\n",
    "  \n",
    "  token_list = word_tokenize(tweet)\n",
    "\n",
    "  if not keep_punct:\n",
    "    token_list = [token for token in token_list\n",
    "                  if token not in string.punctuation]\n",
    "\n",
    "  if not keep_alnum:\n",
    "    token_list = [token for token in token_list if token.isalpha()]\n",
    "  \n",
    "  if not keep_stop:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.discard('not')\n",
    "    token_list = [token for token in token_list if not token in stop_words]\n",
    "\n",
    "  return token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_z5OuuxY6L9b"
   },
   "source": [
    "* Test the function with a particular tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6LDWcJ99E6Z2"
   },
   "outputs": [],
   "source": [
    "tweet = \"these are 5 different words!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gXyaftGrEvkC"
   },
   "outputs": [],
   "source": [
    "print(\"Tweet tokens: {}\".format(custom_tokenize(tweet, \n",
    "                                                keep_punct=True, \n",
    "                                                keep_alnum=True, \n",
    "                                                keep_stop=True)))\n",
    "print(\"Tweet tokens: {}\".format(custom_tokenize(tweet, keep_stop=True)))\n",
    "print(\"Tweet tokens: {}\".format(custom_tokenize(tweet, keep_alnum=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIY4CKCcTyEo"
   },
   "source": [
    "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=250px>\n",
    "\n",
    "## **2.4** Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoizpNg82nK_"
   },
   "source": [
    "More information about `NLTK.stem`? Head over to: https://www.nltk.org/api/nltk.stem.html\n",
    "\n",
    "* Import different libraries and modules used for stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5oaU_cd_Wz0u"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lb9Joc083a2i"
   },
   "source": [
    "* List of tokens to stem (remember that we stem tokens and not entire sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sTraXL0yy1Up"
   },
   "outputs": [],
   "source": [
    "tokens = [\"manager\", \"management\", \"managing\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v35wUbss5fnI"
   },
   "source": [
    "* Stemmers can be defined by directly using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o8QDL-05tMj5"
   },
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "snoball_stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5j_BLPU5ptp"
   },
   "source": [
    "* Create a `stem_tokens` function that takes the list of tokens as input and returns a list of stemmed tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CetNdOeRTyjj"
   },
   "outputs": [],
   "source": [
    "def stem_tokens(tokens, stemmer):\n",
    "  token_list = []\n",
    "  for token in tokens:\n",
    "    token_list.append(stemmer.stem(token))\n",
    "  return token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swSFzfQI6PQ3"
   },
   "source": [
    "* Print the different results and compare the stemmed tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YDZqSc43tKjI"
   },
   "outputs": [],
   "source": [
    "print(\"Porter stems: {}\".format(stem_tokens(tokens, porter_stemmer)))\n",
    "print(\"Lancaster stems: {}\".format(stem_tokens(tokens, lancaster_stemmer)))\n",
    "print(\"Snowball stems: {}\".format(stem_tokens(tokens, snoball_stemmer)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H102rFNu6VzW"
   },
   "source": [
    "* Check over-stemming and under-stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9beBm8O0Gpp"
   },
   "outputs": [],
   "source": [
    "tokens = [\"international\", \"companies\", \"had\", \"interns\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pGGhCVph1VDp"
   },
   "outputs": [],
   "source": [
    "print(\"Porter stems: {}\".format(stem_tokens(tokens, porter_stemmer)))\n",
    "print(\"Lancaster stems: {}\".format(stem_tokens(tokens, lancaster_stemmer)))\n",
    "print(\"Snowball stems: {}\".format(stem_tokens(tokens, snoball_stemmer)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62AsuQu6UIEn"
   },
   "source": [
    "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=250px>\n",
    "\n",
    "## **2.5** Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xk8rZjBEzcaI"
   },
   "source": [
    "More information about WordNet? Head over to https://wordnet.princeton.edu/\n",
    "\n",
    "* Import different libraries and modules used for lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lUdQlhSQUL7T"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QgJuRIZf7yWQ"
   },
   "source": [
    "* List of tokens to lemmatize (remember that we lemmatize tokens and not entire sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kljxJlz0yJ8H"
   },
   "outputs": [],
   "source": [
    "tokens = [\"international\", \"companies\", \"had\", \"interns\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQjP0xoH2NcZ"
   },
   "source": [
    "* Part of Speech (POS) tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oWHJMdHH18le"
   },
   "outputs": [],
   "source": [
    "word_type = {\"international\": wordnet.ADJ, \n",
    "             \"companies\": wordnet.NOUN, \n",
    "             \"had\": wordnet.VERB, \n",
    "             \"interns\": wordnet.NOUN\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e30o3j5N72sw"
   },
   "source": [
    "* Create the lemmatizer by using the `WordNet` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-AAGLN4UWTO"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9pDpdOo78jH"
   },
   "source": [
    "* Create a `lemmatize_tokens` function that takes the list of tokens as input and returns a list of lemmatized tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D6Kiw9xOZE-F"
   },
   "outputs": [],
   "source": [
    "def lemmatize_tokens(tokens, word_type, lemmatizer):\n",
    "  token_list = []\n",
    "  for token in tokens:\n",
    "    token_list.append(lemmatizer.lemmatize(token, word_type[token]))\n",
    "  return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2nnvMrk_UYZ1"
   },
   "outputs": [],
   "source": [
    "print(\"Tweet lemma: {}\".format(\n",
    "    lemmatize_tokens(tokens, word_type, lemmatizer)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTervSXj3zkF"
   },
   "source": [
    "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=250px>\n",
    "\n",
    "## **2.6** Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ag9YSiUE8I03"
   },
   "source": [
    "* Long and complex tweet to be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "COBX24hLZiZn"
   },
   "outputs": [],
   "source": [
    "complex_tweet = r\"\"\"RT @AIOutsider : he looooook, \n",
    "THis is a big and complex TWeet!!! üëç ... \n",
    "We'd be glad if you couldn't normalize it! \n",
    "Check https://t.co/7777 and LET ME KNOW!!! #NLP #Fun\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pRABF3t8MO0"
   },
   "source": [
    "* Create a custom `process_tweet` function that can be used to process tweets end-to-end\n",
    "* **Note**: this function will be used as a base for the following sections, so be careful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iiQofxMv37Ik"
   },
   "outputs": [],
   "source": [
    "def process_tweet(tweet, verbose=False):\n",
    "  if verbose: print(\"Initial tweet: {}\".format(tweet))\n",
    "\n",
    "  ## Twitter Features\n",
    "  tweet = replace_retweet(tweet) # replace retweet\n",
    "  tweet = replace_user(tweet, \"\") # replace user tag\n",
    "  tweet = replace_url(tweet) # replace url\n",
    "  tweet = replace_hashtag(tweet) # replace hashtag\n",
    "  if verbose: print(\"Post Twitter processing tweet: {}\".format(tweet))\n",
    "\n",
    "  ## Word Features\n",
    "  tweet = to_lowercase(tweet) # lower case\n",
    "  tweet = fix_contractions(tweet) # replace contractions\n",
    "  tweet = punct_repetition(tweet) # replace punctuation repetition\n",
    "  tweet = word_repetition(tweet) # replace word repetition\n",
    "  tweet = demojize(tweet) # replace emojis\n",
    "  if verbose: print(\"Post Word processing tweet: {}\".format(tweet))\n",
    "\n",
    "  ## Tokenization & Stemming\n",
    "  tokens = custom_tokenize(tweet, keep_alnum=False, keep_stop=False) # tokenize\n",
    "  stemmer = SnowballStemmer(\"english\") # define stemmer\n",
    "  stem = stem_tokens(tokens, stemmer) # stem tokens\n",
    "\n",
    "  return stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKBv4elc81PC"
   },
   "source": [
    "* Test your `process_tweet` function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Xb146QF4uhj"
   },
   "outputs": [],
   "source": [
    "print(process_tweet(complex_tweet, verbose=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hy48UzeYHFoP"
   },
   "source": [
    "* Look at some more examples! \n",
    "* **Note:** it's totally possible you encounter some strange tweet processing (happens if the original tweet is initially strangely written) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "26lxJnf2K7zd"
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XBa0HUtM4x0n"
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "  tweet_id = random.randint(0,len(df))\n",
    "  tweet = df.iloc[tweet_id][\"tweet_text\"]\n",
    "  print(process_tweet(tweet, verbose=True))\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "haxwoF1IM1f1"
   },
   "source": [
    "That's it for this section! \n",
    "\n",
    "See you in `Section 3` where we will talk about Text Representation and get close to predicting real tweet sentiment!\n",
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=view&id=1-PExlpxdip_2t7wN9Ru8Sn_jTTxj3MlA' width=100px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1ScK4-f_cJB"
   },
   "source": [
    "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=650px>\n",
    "\n",
    "# `Section 3` Text Representation </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgEa9cqUzoEK"
   },
   "source": [
    "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=250px>\n",
    "\n",
    "## **3.1** Processing Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIz-oZSEqJB7"
   },
   "source": [
    "* Install the `Scikit-Learn` package which is very useful for a lot of different ML tasks. \n",
    "* **Note:** make sure it is installed and up-to-date (once installed/updated, you might be asked to reload Colab). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T1oZA6AjqC3X"
   },
   "outputs": [],
   "source": [
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjvFLUXi_Ukv"
   },
   "source": [
    "* Apply `process_tweet` function created in section 2 to the entire DataFrame\n",
    "* Convert sentiment to 1 for \"positive\" and 0 for \"negative\" sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0_4R6tNqcAg"
   },
   "outputs": [],
   "source": [
    "df[\"tokens\"] = df[\"tweet_text\"].apply(process_tweet)\n",
    "df[\"tweet_sentiment\"] = df[\"sentiment\"].apply(lambda i: 1\n",
    "                                              if i == \"positive\" else 0)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6YHtN5xwqiGM"
   },
   "source": [
    "* Convert DataFrame to two lists: one for the tweet tokens (X) and one for the tweet sentiment (y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ru_1XwdIqgXH"
   },
   "outputs": [],
   "source": [
    "X = df[\"tokens\"].tolist()\n",
    "y = df[\"tweet_sentiment\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dm6k_K81i1cG"
   },
   "outputs": [],
   "source": [
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgcJewZTobFk"
   },
   "source": [
    "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=250px>\n",
    "\n",
    "## **3.2** Positive/Negative Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBpw9KYPHcz5"
   },
   "source": [
    "* Corpus of tweet tokens used for the first method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VygUsA2VbNQv"
   },
   "outputs": [],
   "source": [
    "corpus = [[\"i\", \"love\", \"nlp\"],\n",
    "          [\"i\", \"miss\", \"you\"],\n",
    "          [\"i\", \"love\", \"you\"],\n",
    "          [\"you\", \"are\", \"happy\", \"to\", \"learn\"],\n",
    "          [\"i\", \"lost\", \"my\", \"computer\"],\n",
    "          [\"i\", \"am\", \"so\", \"sad\"]]\n",
    "\n",
    "sentiment = [1, 0, 1, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLdGueKbv9o9"
   },
   "source": [
    "\n",
    "* Create a `build_freqs` function used to build a dictionnary with the word and sentiment as index and the count of occurence as value\n",
    "\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th>Word</th>\n",
    "    <th>Positive</th>\n",
    "    <th>Negative</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>love</td>\n",
    "    <td>dict[(love, 1)]</td>\n",
    "    <td>dict[(love, 0)]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>lost</td>\n",
    "    <td>dict[(lost, 1)]</td>\n",
    "    <td>dict[(lost, 0)]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>happy</td>\n",
    "    <td>dict[(happy, 1)]</td>\n",
    "    <td>dict[(happy, 0)]</td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PSSP3EFyp9Nr"
   },
   "outputs": [],
   "source": [
    "def build_freqs(tweet_list, sentiment_list):\n",
    "  freqs = {}\n",
    "  for tweet, sentiment in zip(tweet_list, sentiment_list):\n",
    "    for word in tweet:\n",
    "      pair = (word, sentiment)\n",
    "      if pair in freqs:\n",
    "        freqs[pair] += 1\n",
    "      else:\n",
    "        freqs[pair] = 1\n",
    "  return freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpMlVUPPx136"
   },
   "source": [
    "* Build the frequency dictionnary on the corpus by using the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S58DffMwi5vN"
   },
   "outputs": [],
   "source": [
    "freqs = build_freqs(corpus, sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pb0BvLORtJql"
   },
   "outputs": [],
   "source": [
    "print(freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64pK3pBlx_Hn"
   },
   "source": [
    "* Build the frequency dictionnary on the entire dataset by using the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6vnVv4TOi63Y"
   },
   "outputs": [],
   "source": [
    "freqs_all = build_freqs(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mvafc_1fsktr"
   },
   "outputs": [],
   "source": [
    "print(\"Frequency of word 'love' in + tweets: {}\".format(freqs_all[(\"love\", 1)]))\n",
    "print(\"Frequency of word 'love' in - tweets: {}\".format(freqs_all[(\"love\", 0)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KosMwShRyUTW"
   },
   "source": [
    "* Create a `tweet_to_freqs` function used to convert tweets to a 2-d array by using the frequency dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6QoyYL6wtTBP"
   },
   "outputs": [],
   "source": [
    "def tweet_to_freq(tweet, freqs):\n",
    "  x = np.zeros((2,))\n",
    "  for word in tweet:\n",
    "    if (word, 1) in freqs:\n",
    "      x[0] += freqs[(word, 1)]\n",
    "    if (word, 0) in freqs:\n",
    "      x[1] += freqs[(word, 0)]\n",
    "  return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjGynFT_JOKY"
   },
   "source": [
    "* Print the 2-d vector by using the `tweet_to_freqs` function and the *corpus* dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Nrowt5T0ZQl"
   },
   "outputs": [],
   "source": [
    "print(tweet_to_freq([\"i\", \"love\", \"nlp\"], freqs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0csoh9UJWgP"
   },
   "source": [
    "* Print the 2-d vector by using the `tweet_to_freqs` function and the *dataset* dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PTmNWLcD2QGn"
   },
   "outputs": [],
   "source": [
    "print(tweet_to_freq([\"i\", \"love\", \"nlp\"], freqs_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WsLNWRrh2XQ9"
   },
   "source": [
    "* Plot word vectors in a chart and see where they locate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JL3E5vRrxJrZ"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (8, 8))\n",
    "\n",
    "word1 = \"happi\"\n",
    "word2 = \"sad\"\n",
    "\n",
    "def word_features(word, freqs):\n",
    "  x = np.zeros((2,))\n",
    "  if (word, 1) in freqs:\n",
    "    x[0] = np.log(freqs[(word, 1)] + 1)\n",
    "  if (word, 0) in freqs:\n",
    "    x[1] = np.log(freqs[(word, 0)] + 1)\n",
    "  return x\n",
    "\n",
    "x_axis = [word_features(word, freqs_all)[0] for word in [word1, word2]]\n",
    "y_axis = [word_features(word, freqs_all)[1] for word in [word1, word2]]\n",
    "\n",
    "ax.scatter(x_axis, y_axis)  \n",
    "\n",
    "plt.xlabel(\"Log Positive count\")\n",
    "plt.ylabel(\"Log Negative count\")\n",
    "\n",
    "ax.plot([0, 9], [0, 9], color = 'red')\n",
    "plt.text(x_axis[0], y_axis[0], word1)\n",
    "plt.text(x_axis[1], y_axis[1], word2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_a_uGCpQ_nu0"
   },
   "source": [
    "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=250px>\n",
    "\n",
    "## **3.3** Bag of Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wh-4DbOFJlAn"
   },
   "source": [
    "* Corpus of tweet tokens used for the second method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SomLbPUibDa9"
   },
   "outputs": [],
   "source": [
    "corpus = [[\"love\", \"nlp\"],\n",
    "          [\"miss\", \"you\"],\n",
    "          [\"hate\", \"hate\", \"hate\", \"love\"],\n",
    "          [\"happy\", \"love\", \"hate\"],\n",
    "          [\"i\", \"lost\", \"my\", \"computer\"],\n",
    "          [\"i\", \"am\", \"so\", \"sad\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEe5w7SPso2e"
   },
   "source": [
    "* Import `CountVectorizer` from the Scikit-learn Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UjjZkgHCpw_m"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8a5WqAGSJ0KN"
   },
   "source": [
    "* Create a `fit_cv` function used to build the Bag-of-Words vectorizer with the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hEzNadu6pwYh"
   },
   "outputs": [],
   "source": [
    "def fit_cv(tweet_corpus):\n",
    "  cv_vect = CountVectorizer(tokenizer=lambda x: x,\n",
    "                            preprocessor=lambda x: x)\n",
    "  cv_vect.fit(tweet_corpus)\n",
    "  return cv_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z07FwlXUJ7ld"
   },
   "source": [
    "* Use the `fit_cv` function to fit the vectorizer on the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xed7_qlKwoX9"
   },
   "outputs": [],
   "source": [
    "cv_vect = fit_cv(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0PJfMk2sthJ"
   },
   "source": [
    "* Get the vectorizer features (matrix columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJcalFLxjIEx"
   },
   "outputs": [],
   "source": [
    "ft = cv_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e-nxkR7VfWii"
   },
   "outputs": [],
   "source": [
    "print(\"There are {} features in this corpus\".format(len(ft)))\n",
    "print(ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6IueKWpsswRn"
   },
   "source": [
    "* Convert the corpus to a matrix by using the vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZ-mcZScBdtb"
   },
   "outputs": [],
   "source": [
    "cv_mtx = cv_vect.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iSf9F_jgKRnr"
   },
   "source": [
    "* Print the matrix shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXm86BaJp0yB"
   },
   "outputs": [],
   "source": [
    "print(\"Matrix shape is: {}\".format(cv_mtx.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHWE38UMKVOG"
   },
   "source": [
    "* Convert the matrix to an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TBH3Hy60gQB4"
   },
   "outputs": [],
   "source": [
    "cv_mtx.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvnrCpxvKaMR"
   },
   "source": [
    "* Transform a new tweet by using the vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLVG9xwdwzuN"
   },
   "outputs": [],
   "source": [
    "new_tweet = [[\"lost\", \"lost\", \"miss\", \"miss\"]]\n",
    "cv_vect.transform(new_tweet).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LMuCe5KsxyR_"
   },
   "outputs": [],
   "source": [
    "unknown_tweet = [[\"John\", \"drives\", \"cars\"]]\n",
    "cv_vect.transform(unknown_tweet).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0sa_FL6vn79E"
   },
   "source": [
    "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=250px>\n",
    "\n",
    "## **3.4** Term Frequency ‚Äì Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOZSwJ1hKyAT"
   },
   "source": [
    "* Corpus of tweet tokens used for the third method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7ybyRRPZC-L"
   },
   "outputs": [],
   "source": [
    "corpus = [[\"love\", \"nlp\"],\n",
    "          [\"miss\", \"you\"],\n",
    "          [\"hate\", \"hate\", \"hate\", \"love\"],\n",
    "          [\"happy\", \"love\", \"hate\"],\n",
    "          [\"i\", \"lost\", \"my\", \"computer\"],\n",
    "          [\"i\", \"am\", \"so\", \"sad\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NE5zVtjsK6Ta"
   },
   "source": [
    "* Import `TfidfVectorizer` from the Scikit-learn Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HlBvj6pARLH1"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQgGfCwEK-mI"
   },
   "source": [
    "* Create a `fit_tfidf` function used to build the TF-IDF vectorizer with the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9NUd39ZsRQPI"
   },
   "outputs": [],
   "source": [
    "def fit_tfidf(tweet_corpus):\n",
    "  tf_vect = TfidfVectorizer(preprocessor=lambda x: x,\n",
    "                            tokenizer=lambda x: x)\n",
    "  tf_vect.fit(tweet_corpus)\n",
    "  return tf_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tac2oPSRLErQ"
   },
   "source": [
    "* Use the `fit_cv` function to fit the vectorizer on the corpus, and transform the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NjmZhliftA_Q"
   },
   "outputs": [],
   "source": [
    "tf_vect = fit_tfidf(corpus)\n",
    "tf_mtx = tf_vect.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kt1eOml3LMas"
   },
   "source": [
    "* Get the vectorizer features (matrix columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zRqHhf3njSEe"
   },
   "outputs": [],
   "source": [
    "ft = tf_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tywqkwWPlYpc"
   },
   "outputs": [],
   "source": [
    "print(\"There are {} features in this corpus\".format(len(ft)))\n",
    "print(ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BGtKec9LPNg"
   },
   "source": [
    "* Print the matrix shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XtamuqDjlxmE"
   },
   "outputs": [],
   "source": [
    "print(tf_mtx.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usbtYKPCLTBN"
   },
   "source": [
    "* Convert the matrix to an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVVbtSxqlxVi"
   },
   "outputs": [],
   "source": [
    "tf_mtx.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NxWpupYkLVxl"
   },
   "source": [
    "* Transform a new tweet by using the vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87JJ_go4uFO5"
   },
   "outputs": [],
   "source": [
    "new_tweet = [[\"I\", \"hate\", \"nlp\"]]\n",
    "tf_vect.transform(new_tweet).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAgMDeTwSkog"
   },
   "source": [
    "That's it for this section! \n",
    "\n",
    "See you in `Section 4` where we finally build our sentiment model!\n",
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=view&id=1-PExlpxdip_2t7wN9Ru8Sn_jTTxj3MlA' width=100px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7H4tg1lhoBxR"
   },
   "source": [
    "\n",
    "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=650px>\n",
    "\n",
    "# `Section 4` Sentiment Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjAoXb4XsxgN"
   },
   "source": [
    "## Helper function\n",
    "\n",
    "This function will be used to plot the confusion matrix for the different models we will create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TBiVy9LSnlVN"
   },
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "\n",
    "def plot_confusion(cm):\n",
    "  plt.figure(figsize = (5,5))\n",
    "  sn.heatmap(cm, annot=True, cmap=\"Blues\", fmt='.0f')\n",
    "  plt.xlabel(\"Prediction\")\n",
    "  plt.ylabel(\"True value\")\n",
    "  plt.title(\"Confusion Matrix\")\n",
    "  return sn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePstP2McY2Ql"
   },
   "source": [
    "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=250px>\n",
    "\n",
    "## **4.1** Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfXwu-g4onVk"
   },
   "source": [
    "* Check what X and y looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YMqL7yh1Dt8t"
   },
   "outputs": [],
   "source": [
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GnJENLcJooVj"
   },
   "source": [
    "* Import the `train_test_split` function from the Scikit-Learn package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vkeEyh9TeTT3"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgDUiv-po5LZ"
   },
   "source": [
    "* Use the `train_test_split` function to split arrays of X and y into training and testing variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XdKNxWTXDvLM"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    random_state=0,\n",
    "                                                    train_size=0.80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4uKnKwkpG_J"
   },
   "source": [
    "* Print the size of these news variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_fUaBFavoBTG"
   },
   "outputs": [],
   "source": [
    "print(\"Size of X_train: {}\".format(len(X_train)))\n",
    "print(\"Size of y_train: {}\".format(len(y_train)))\n",
    "print(\"\\n\")\n",
    "print(\"Size of X_test: {}\".format(len(X_test)))\n",
    "print(\"Size of y_test: {}\".format(len(y_test)))\n",
    "print(\"\\n\")\n",
    "print(\"Train proportion: {:.0%}\".format(len(X_train)/\n",
    "                                        (len(X_train)+len(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVouoqmepP0N"
   },
   "source": [
    "* Print random tweets, just to verify everything goes as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T5nWVNZzobfd"
   },
   "outputs": [],
   "source": [
    "id = random.randint(0,len(X_train))\n",
    "print(\"Train tweet: {}\".format(X_train[id]))\n",
    "print(\"Sentiment: {}\".format(y_train[id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pV5EWbQpIOp"
   },
   "source": [
    "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=250px>\n",
    "\n",
    "## **4.2** Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vvELqpBA62u"
   },
   "source": [
    "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **4.2.1** Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8DrY2m6L0zq"
   },
   "source": [
    "* Import the `LogisticRegression` model from Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVCZ2jEcb_Kx"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8oqTzbsL5N7"
   },
   "source": [
    "* Create a `fit_lr` function used to fit a Logistic Regression model on X and y *training* data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IN8x-HdwcC1K"
   },
   "outputs": [],
   "source": [
    "def fit_lr(X_train, y_train):\n",
    "  model = LogisticRegression()\n",
    "  model.fit(X_train, y_train)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWD8LNbJDHG9"
   },
   "source": [
    "\n",
    "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px>  **4.2.2** Pos/Neg Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bqSbvMxMDk8"
   },
   "source": [
    "* Use the `build_freqs` function on training data to create a frequency dictionnary\n",
    "* Use the frequency dictionnary together with the `tweet_to_freq` function to convert X_train and X_test data to 2-d vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hKDhO2RKBafD"
   },
   "outputs": [],
   "source": [
    "freqs = build_freqs(X_train, y_train)\n",
    "X_train_pn = [tweet_to_freq(tweet, freqs) for tweet in X_train]\n",
    "X_test_pn = [tweet_to_freq(tweet, freqs) for tweet in X_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FgQYsodMl1c"
   },
   "source": [
    "* Fit the Logistic Regression model on training data by using the `fit_lr` function\n",
    "* Print the model coefficients (betas and intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mgXgI2QcjYyv"
   },
   "outputs": [],
   "source": [
    "model_lr_pn = fit_lr(X_train_pn, y_train)\n",
    "print(model_lr_pn.coef_, model_lr_pn.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZtyVpIDZBRr"
   },
   "source": [
    "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **4.2.3** Count Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-itafPb7M2O-"
   },
   "source": [
    "* Use the `fit_cv` function on training data to build the Bag-of-Words vectorizer\n",
    "* Transform X_train and X_test data by using the vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zXba4ZPaYsb5"
   },
   "outputs": [],
   "source": [
    "cv = fit_cv(X_train)\n",
    "X_train_cv = cv.transform(X_train)\n",
    "X_test_cv = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGDgMoYMNGA4"
   },
   "source": [
    "* Fit the Logistic Regression model on training data by using the `fit_lr` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xarmCHAIZMty"
   },
   "outputs": [],
   "source": [
    "model_lr_cv = fit_lr(X_train_cv, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vo7WerftZV4t"
   },
   "source": [
    "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **4.2.4** TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zm27oDZHNMxd"
   },
   "source": [
    "* Use the `fit_cv` function on training data to build the Bag-of-Words vectorizer\n",
    "* Transform X_train and X_test data by using the vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y2Ex6PkLZXwC"
   },
   "outputs": [],
   "source": [
    "tf = fit_tfidf(X_train)\n",
    "X_train_tf = tf.transform(X_train)\n",
    "X_test_tf = tf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdkMm9TWNVTM"
   },
   "source": [
    "* Fit the Logistic Regression model on training data by using the `fit_lr` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JfXA_WE7ZgCO"
   },
   "outputs": [],
   "source": [
    "model_lr_tf = fit_lr(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTZWCw7CuAqy"
   },
   "source": [
    "<img src='https://drive.google.com/uc?export=view&id=1GYj-wj-so8jQ9-VDz1ayehgVh39Jmd4H' width=250px>\n",
    "\n",
    "## **4.3** Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GlWsYS6GNYi8"
   },
   "source": [
    "* Import the `accuracy score` and `confusion matrix` from Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pv_HXA9ttu4I"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-OBeBb4vf-Y"
   },
   "source": [
    "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **4.3.1** Positive/Negative Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNAY6ENLNgla"
   },
   "source": [
    "* Use the fitted `model_lr_pn` (positive/negative frequencies) to predict X_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WGSRknqlj6fu"
   },
   "outputs": [],
   "source": [
    "y_pred_lr_pn = model_lr_pn.predict(X_test_pn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D647C9ORj7Ty"
   },
   "source": [
    "* Print the model accuracy by comparing predictions and real sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h4Mw_VHwj6Qp"
   },
   "outputs": [],
   "source": [
    "print(\"LR Model Accuracy: {:.2%}\".format(accuracy_score(y_test, y_pred_lr_pn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEtKfgdij-cU"
   },
   "source": [
    "* Plot the confusion matrix by using the `plot_confusion` helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZMFVLs_vAO03"
   },
   "outputs": [],
   "source": [
    "plot_confusion(confusion_matrix(y_test, y_pred_lr_pn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzTQksefvlxJ"
   },
   "source": [
    "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **4.3.2** Count Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cP_bueksN2cS"
   },
   "source": [
    "* Use the fitted `model_lr_cv` (Bag-of-words) to predict X_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "suYa_OYgZPq1"
   },
   "outputs": [],
   "source": [
    "y_pred_lr_cv = model_lr_cv.predict(X_test_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hpn1YUFokKXm"
   },
   "source": [
    "* Print the model accuracy by comparing predictions and real sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WAOncwPekK1_"
   },
   "outputs": [],
   "source": [
    "print(\"LR Model Accuracy: {:.2%}\".format(accuracy_score(y_test, y_pred_lr_cv)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3_6Fg_zkLVm"
   },
   "source": [
    "* Plot the confusion matrix by using the `plot_confusion` helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HlaWIWnNkL1O"
   },
   "outputs": [],
   "source": [
    "plot_confusion(confusion_matrix(y_test, y_pred_lr_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3TxYFxnduFCH"
   },
   "source": [
    "### <img src='https://drive.google.com/uc?export=view&id=1aAdtCrMe6SORoGAGtjOVM0UIxDFH9Thq' width=50px> **4.3.3** TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iZdLPwcOAk8"
   },
   "source": [
    "* Use the fitted `model_lr_tf` (TF-IDF) to predict X_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o4MmLzcCZvto"
   },
   "outputs": [],
   "source": [
    "y_pred_lr_tf = model_lr_tf.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Hn_RKShkVcm"
   },
   "source": [
    "* Print the model accuracy by comparing predictions and real sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GD6mO1eJkV3m"
   },
   "outputs": [],
   "source": [
    "print(\"LR Model Accuracy: {:.2%}\".format(accuracy_score(y_test, y_pred_lr_tf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPojrJ56kWKL"
   },
   "source": [
    "* Plot the confusion matrix by using the `plot_confusion` helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kt_FBSmbkW0b"
   },
   "outputs": [],
   "source": [
    "plot_confusion(confusion_matrix(y_test, y_pred_lr_tf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUjQh8vnPld9"
   },
   "source": [
    "## **4.4** Mini-Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FRX-suJOIqH"
   },
   "source": [
    "* Final tweet used to check if the model works as well as expected\n",
    "* **Note:** don't hesitate to input your own tweet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bpybhropPsSe"
   },
   "outputs": [],
   "source": [
    "your_tweet = \"\"\"RT @AIOutsider: tune in for more amazing NLP content! \n",
    "And don't forget to visit https://AIOutsider.com ...\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nmPPbkPOSda"
   },
   "source": [
    "* Create a `predict_tweet` function used to pre-process, transform and predict tweet sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3LKbEteQOXy"
   },
   "outputs": [],
   "source": [
    "def predict_tweet(tweet):\n",
    "  processed_tweet = process_tweet(tweet)\n",
    "  transformed_tweet = tf.transform([processed_tweet])\n",
    "  prediction = model_lr_tf.predict(transformed_tweet)\n",
    "\n",
    "  if prediction == 1:\n",
    "    return \"Prediction is positive sentiment\"\n",
    "  else:\n",
    "    return \"Prediction is negative sentiment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9M-pJ6GOaHw"
   },
   "source": [
    "* ... Predict your tweet sentiment by using the `predict_tweet` function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YThD6uAmMFEZ"
   },
   "outputs": [],
   "source": [
    "predict_tweet(your_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5TjajsKv7Ha"
   },
   "source": [
    "# Thank you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4X5JPSycSwZR"
   },
   "source": [
    "That's it for this course! I hope you enjoyed it as much as I did! Most importantly, I hope you learned some new things about Text Mining, NLP and Sentiment Analysis.\n",
    "\n",
    "See you next time!\n",
    "\n",
    "**AI_Outsider**\n",
    "\n",
    "Don't forget to visit https://AiOutsider.com for more!\n",
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=view&id=1-PExlpxdip_2t7wN9Ru8Sn_jTTxj3MlA' width=100px>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "DHH-1YKcIuRW",
    "0LKMid597qjL",
    "7URUz-xz9otD"
   ],
   "name": "Colab Notebook - Section 4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
